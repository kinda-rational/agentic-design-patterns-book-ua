# Розділ 14: Вилучення знань (RAG)

LLM демонструють значні можливості у генеруванні людиноподібного тексту. Однак їхня база знань зазвичай обмежена даними, на яких вони були тренувані, що обмежує їхній доступ до інформації в реальному часі, специфічних корпоративних даних чи високоспеціалізованих деталей. Вилучення знань (RAG, або Retrieval Augmented Generation) вирішує це обмеження. RAG дозволяє LLM отримувати доступ до зовнішньої, актуальної та контекстно-специфічної інформації та інтегрувати її, таким чином підвищуючи точність, релевантність та фактичну основу їхніх результатів.

Для AI-агентів це надзвичайно важливо, оскільки дозволяє їм засновувати свої дії та відповіді на даних реального часу, що можуть бути перевірені, що виходять за межі їх статичного тренування. Ця можливість дозволяє їм точно виконувати складні завдання, такі як доступ до останніх корпоративних політик для відповіді на конкретне питання або перевірка поточних запасів перед розміщенням замовлення. Інтегруючи зовнішні знання, RAG трансформує агентів з простих розмовників на ефективні, засновані на даних інструменти, здатні виконувати значиму роботу.

# Огляд паттерну вилучення знань (RAG)

Паттерн вилучення знань (RAG) значно розширює можливості LLM, надаючи їм доступ до зовнішніх баз знань перед генеруванням відповіді. Замість покладання виключно на свої внутрішні, попередньо тренувані знання, RAG дозволяє LLM "шукати" інформацію, подібно до того, як людина може звернутися до книги або пошукати в інтернеті. Цей процес дає LLM можливість надавати більш точні, актуальні та перевіряємі відповіді.

Коли користувач задає питання чи дає промпт AI-системі, що використовує RAG, запит не надсилається безпосередньо до LLM. Замість цього система спочатку переглядає розширену зовнішню базу знань — добре організовану бібліотеку документів, баз даних чи веб-сторінок — у пошуках релевантної інформації. Цей пошук не є простим зіставленням ключових слів; це "семантичний пошук", що розуміє намір користувача та значення, що стоїть за його словами. Цей початковий пошук вилучає найбільш відповідні фрагменти чи "чанки" інформації. Ці вилучені частини потім "доповнюють" або додаються до оригінального промпта, створюючи більш багатий, більш інформований запит. Нарешті, цей розширений промпт надсилається до LLM. З цим додатковим контекстом LLM може генерувати відповідь, яка не тільки вільна й природна, але й фактично засновано на вилучених даних.

Фреймворк RAG надає кілька значних переваг. Він дозволяє LLM отримати доступ до актуальної інформації, таким чином подолавши обмеження їх статичних даних тренування. Цей підхід також зменшує ризик "галюцинацій" — генерування хибної інформації — засновуючи відповіді на перевіряємих даних. Крім того, LLM можуть використовувати спеціалізовані знання, виявлені у внутрішніх корпоративних документах чи wiki. Життєво важливою перевагою цього процесу є здатність надати "цитати", що вказують на точне джерело інформації, таким чином підвищуючи надійність та перевіряємість відповідей AI.

Для повного розуміння того, як функціонує RAG, важливо розуміти кілька основних концепцій (див. Рис.1):

**Вбудовування (Embeddings)**: У контексті LLM, вбудовування - це числові представлення тексту, такого як слова, фрази чи цілі документи. Ці представлення мають форму вектора, який є списком чисел. Ключова ідея полягає у тому, щоб зафіксувати семантичне значення та взаємозв'язки між різними фрагментами тексту у математичному просторі. Слова чи фрази з подібними значеннями матимуть вбудовування, що розташовані ближче один до одного у цьому векторному просторі. Наприклад, уявіть простий 2D-графік. Слово "кіт" може бути представлено координатами (2, 3), тоді як "кошеня" буде дуже близько на (2.1, 3.1). На контрасті слово "автомобіль" матиме віддалену координату, таку як (8, 1), що відбиває його інше значення. Насправді ці вбудовування розташовані у набагато більш высокомирному просторі з сотнями чи навіть тисячами вимірів, що дозволяє дуже нюансоване розуміння мови.

**Текстова подібність:** Текстова подібність означає міру того, наскільки подібні два фрагменти тексту. Це може бути на поверхневому рівні, розглядаючи перехрестя слів (лексична подібність), або на більш глибокому, засновано на значенні рівні. У контексті RAG текстова подібність є вирішальною для пошуку найбільш релевантної інформації у базі знань, що відповідає запиту користувача. Наприклад, розглянемо речення: "Яка столиця Франції?" та "Яке місто є столицею Франції?". Хоча формулювання відрізняється, вони задають те саме питання. Хороша модель текстової подібності розпізнавала б це та присвоювала б високу оцінку подібності цим двом реченням, навіть якщо вони поділяють тільки кілька слів. Це часто обчислюється, використовуючи вбудовування тексту.

**Семантична подібність та відстань:** Семантична подібність - це більш передова форма текстової подібності, що фокусується винятково на значенні та контексті тексту, а не тільки на використаних словах. Вона спрямована на розуміння того, передають ли два фрагменти тексту один і той же концепт чи ідею. Семантична відстань є оберненням; висока семантична подібність означає низьку семантичну відстань, і навпаки. У RAG семантичний пошук покладається на пошук документів з найменшою семантичною відстанню до запиту користувача. Наприклад, фрази "пухнастий кішачий супутник" та "домашня кішка" не мають спільних слів, крім артиклю. Однак модель, що розуміє семантичну подібність, розпізнавала б, що вони відносяться до однієї й тієї ж, та вважала б їх дуже подібними. Це відбувається тому, що їхні вбудовування були б дуже близько у векторному просторі, указуючи на малу семантичну відстань. Це "розумний пошук", що дозволяє RAG знаходити релевантну інформацію, навіть коли формулювання користувача не точно збігається з текстом у базі знань.

![][image2]

Рис.1: Основні концепції RAG: Розбиття на чанки, Вбудовування та векторна база даних

**Розбиття документів на чанки:** Розбиття на чанки - це процес розділення великих документів на менші, більш керовані частини чи "чанки". Для ефективної роботи RAG-системи вона не може подавати цілі великі документи у LLM. Замість цього вона обробляє ці менші чанки. Спосіб розбиття документів на чанки важливий для збереження контексту та значення інформації. Наприклад, замість розгляду 50-сторінкового посібника користувача як єдиного блоку тексту, стратегія розбиття може розділити його на розділи, абзаци чи навіть речення. Наприклад, розділ "Усунення неполадок" буде окремим чанком від "Посібника з установки". Коли користувач задає питання про конкретну проблему, RAG-система може потім вилучити найбільш релевантний чанк усунення неполадок, а не весь посібник. Це робить процес вилучення швидшим, а інформацію, надану LLM, більш сконцентрованою та релевантною до негайної потреби користувача. Після розбиття документів на чанки RAG-система повинна використовувати техніку вилучення для пошуку найбільш релевантних частин для даного запиту. Основним методом є векторний пошук, який використовує вбудовування та семантичну відстань для пошуку чанків, концептуально подібних до питання користувача. Більш старша, але все ще цінна техніка - це BM25, алгоритм на основі ключових слів, що ранжує чанки на основі частоти термінів без розуміння семантичного значення. Щоб отримати найкраще з обох світів, часто використовуються гібридні підходи пошуку, що поєднують точність ключових слів BM25 з контекстуальним розумінням семантичного пошуку. Такого злиття дозволяє більш надійне та точне вилучення, захоплюючи як буквальні збіги, так і концептуальну релевантність.

**Векторні бази даних:** Векторна база даних - це спеціалізований тип бази даних, призначеної для ефективного зберігання та запиту вбудовувань. Після того як документи розбиті на чанки та трансформовані у вбудовування, ці високомирні вектори зберігаються у векторній базі даних. Традиційні техніки вилучення, такі як пошук на основі ключових слів, чудово знаходять документи, що містять точні слова з запиту, але їм не вистачає глибокого розуміння мови. Вони не розпізнали б, що "пухнастий кішачий супутник" означає "кіт". Тут векторні бази даних перевищують. Вони побудовані спеціально для семантичного пошуку. Зберігаючи текст як числові вектори, вони можуть знаходити результати на основі концептуального значення, а не тільки перехрестя ключових слів. Коли запит користувача також трансформується у вектор, база даних використовує високооптимізовані алгоритми (такі як HNSW - Hierarchical Navigable Small World) для швидкого пошуку серед мільйонів векторів та знаходження тих, що "найближчі" за значенням. Цей підхід значно перевищує RAG, оскільки він виявляє релевантний контекст, навіть якщо формулювання користувача повністю відрізняється від оригінальних документів. По суті, тоді як інші техніки шукають слова, векторні бази даних шукають значення. Ця технологія реалізована у різних формах, від керованих баз даних, таких як Pinecone та Weaviate, до рішень з відкритим кодом, таких як Chroma DB, Milvus та Qdrant. Навіть існуючи бази даних можуть бути доповнені можливостями векторного пошуку, як видно у Redis, Elasticsearch та Postgres (використовуючи розширення pgvector). Основні механізми вилучення часто засновані на бібліотеках, таких як FAISS від Meta AI чи ScaNN від Google Research, які є фундаментальними для ефективності цих систем.

**Проблеми RAG:** Незважаючи на свою потужність, паттерн RAG не позбавлений проблем. Основна проблема виникає, коли інформація, необхідна для відповіді на запит, не обмежена одним чанком, а розподілена кількома частинами документа чи навіть кількома документами. У таких випадках вилучувач може не суміти зібрати весь необхідний контекст, що призводить до неповної чи неточної відповіді. Ефективність системи також сильно залежить від якості процесу розбиття на чанки та вилучення; якщо вилучуються нерелевантні чанки, це може внести шум та збентежити LLM. Крім того, ефективний синтез інформації з потенційно суперечливих джерел залишається значною перешкодою для цих систем. Крім того, ще однією проблемою є те, що RAG вимагає попередньої обробки та зберігання всієї бази знань у спеціалізованих базах даних, таких як векторні чи графові бази даних, що є значним підприємством. Отже, ці знання потребують періодичної синхронізації для збереження актуальності, що є критичною задачею при роботі з розвивальними джерелами, такими як корпоративні wiki. Весь цей процес може мати помітний вплив на продуктивність, збільшуючи затримку, операційні витрати та кількість токенів, використаних у остаточному промпті.

Підсумовуючи, паттерн Retrieval-Augmented Generation (RAG) представляє значний прорив у підвищенні знань та надійності AI. Шляхом безперебійної інтеграції етапу вилучення зовнішніх знань у процес генерування, RAG розв'язує деякі базові обмеження автономних LLM. Основні концепції вбудовувань та семантичної подібності у комбінації з техніками вилучення, такими як пошук за ключовими словами та гібридний пошук, дозволяють системі інтелектуально знаходити релевантну інформацію, яка стає керованою через стратегічне розбиття на чанки. Весь цей процес вилучення забезпечується спеціалізованими векторними базами даних, призначеними для зберігання та ефективного запиту мільйонів вбудовувань у масштабі. Хоча проблеми у вилученні фрагментованої чи суперечливої інформації залишаються, RAG дає LLM можливість виробляти відповіді, які не тільки контекстуально відповідні, але й закріплені у перевіряємих фактах, сприяючи більшій довірі та корисності в AI.

**Graph RAG:** GraphRAG - це продвинута форма Retrieval-Augmented Generation, що використовує граф знань замість простої векторної бази даних для вилучення інформації. Вона відповідає на складні запити, навігуючи явними взаємозв'язками (ребрами) між сутностями даних (вузлами) у цій структурованій базі знань. Ключовою перевагою є її здатність синтезувати відповіді з інформації, фрагментованої по кількох документах, що є частою невдачею традиційного RAG. Розуміючи ці взаємозв'язки, GraphRAG надає більш контекстуально точні та нюансовані відповіді.

Випадки використання включають складний фінансовий аналіз, зв'язування компаній із ринковими подіями, та наукові дослідження для виявлення взаємозв'язків між генами та захворюваннями. Основним недоліком, однак, є значна складність, вартість та експертиза, потрібні для побудови та утримання високоякісного графу знань. Ця установка також менш гнучка та може внести вищу затримку порівняно з простішими системами векторного пошуку. Ефективність системи повністю залежить від якості та повноти базової графової структури. Отже, GraphRAG пропонує превосходне контекстуальне рассудження для складних питань, але за набагато вищу вартість впровадження та утримання. Підсумовуючи, вона перевищує там, де глибокі, взаємопов'язані інсайти більш критичні, ніж швидкість та простота стандартного RAG.

**Agentic RAG:** Еволюція цього паттерну, відома як **Agentic RAG** (див. Рис.2), вводить шар рассудження та прийняття рішень для значного підвищення надійності вилучення інформації. Замість простого вилучення та доповнення, "агент" — спеціалізований компонент AI — діє як критичний привратник та рафіне знань. Замість пасивного прийняття спочатку вилучених даних, цей агент активно допитує їхню якість, релевантність та повноту, як показано у наступних сценаріях.

По-перше, агент превосходит у рефлексії та валідації джерел. Якщо користувач запитує: "Яка політика нашої компанії щодо дистанційної роботи?", стандартний RAG може вилучити блог-пост 2020 року поряд з офіційним документом політики 2025 року. Агент, однак, проаналізував би метаданні документів, розпізнав би політику 2025 року як найбільш актуальну та авторитетну, та відкинув би застарілий блог-пост перед надсиланням правильного контексту LLM для точної відповіді.

![][image1]

Рис.2: Agentic RAG вводить рассуджуючого агента, який активно оцінює, погоджує та уточнює вилучену інформацію для забезпечення більш точної та надійної остаточної відповіді.

По-друге, агент адепт у узгоджуванні конфліктів знань. Уявіть, що фінансовий аналітик запитує: "Яким був бюджет Q1 проекту Альфа?" Система вилучає два документи: початкова пропозиція, що указує бюджет 50,000 євро, та фіналізований фінансовий звіт, що указує його як 65,000 євро. Agentic RAG виявив би це протиріччя, пріоритизував би фінансовий звіт як більш надійне джерело, та надав би LLM перевірену цифру, забезпечуючи, що остаточна відповідь заснована на найбільш точних даних.

По-третє, агент може виконувати багатоступеневе рассудження для синтезу складних відповідей. Якщо користувач запитує: "Як функції та ціноутворення нашого продукту порівнюються з Конкурентом X?", агент розклав би це на окремі під-запити. Він ініціював би окремі пошуки для функцій власного продукту, його ціноутворення, функцій Конкурента X та ціноутворення Конкурента X. Після збору цих окремих шматків інформації агент синтезував би їх у структурований, порівняльний контекст перед подачею його LLM, забезпечуючи всебічну відповідь, яку простий пошук не міг би виробити.

По-четверте, агент може виявити прогалини у знаннях та використати зовнішні інструменти. Припустимо, користувач запитує: "Яка була негайна реакція ринку на наш новий продукт, запущений вчора?" Агент шукає у внутрішній базі знань, яка оновлюється щотижня, та не знаходить релевантної інформації. Розпізнаючи цю прогалину, він може потім активувати інструмент — такий як API живого веб-пошуку — для пошуку свіжих новинних статей та розважань у соціальних мережах. Агент потім використовує цю свіжозібрану зовнішню інформацію для надання актуальної відповіді, долаючи обмеження своєї статичної внутрішної бази даних.

**Проблеми Agentic RAG:** Хоча потужний, агентний шар вводить свій власний набір проблем. Основним недоліком є значне збільшення складності та вартості. Проектування, впровадження та утримання логіки прийняття рішень агента та інтеграцій інструментів вимагає суттєвих інженерних зусиль та додає до обчислювальних витрат. Ця складність також може призвести до збільшеної затримки, оскільки цикли рефлексії, використання інструментів та багатоступеневого рассудження агента займають більше часу, ніж стандартний, прямий процес вилучення. Більше того, сам агент може стати новим джерелом помилок; порочний процес рассудження міг би змусити його застрягти у непродуктивних циклах, неправильно інтерпретувати завдання чи невідповідно відкинути релевантну інформацію, в кінцевому рахунку погіршуючи якість остаточної відповіді.

### Резюме:

**Agentic RAG** представляє складну еволюцію стандартного паттерну вилучення, трансформуючи його з пасивного конвеєра даних на активний, рішучий фреймворк розв'язування проблем. Вбудовуючи шар рассудження, що може оцінювати джерела, узгоджувати конфлікти, розкладати складні питання та використовувати зовнішні інструменти, агенти драматично поліпшують надійність та глибину генерованих відповідей. Це просування робить AI більш надійним та здатним, хоча воно приходить з важливими компромісами в складності системи, затримці та вартості, які мають бути ретельно керовані.

# Практичні застосування та випадки використання

Вилучення знань (RAG) трансформує те, як Large Language Models (LLM) використовуються у різних галузях, підвищуючи їхню здатність надавати більш точні та контекстуально релевантні відповіді.

Застосування включають:

- **Корпоративний пошук та запитання-відповіді:** Організації можуть розробляти внутрішні чат-боти, що відповідають на запити працівників, використовуючи внутрішню документацію, таку як політики HR, технічні посібники та специфікації продуктів. RAG-система вилучає релевантні розділи з цих документів для інформування відповіді LLM.

- **Підтримка клієнтів та служба допомоги:** RAG-засновані системи можуть пропонувати точні та послідовні відповіді на запити клієнтів, посилаючись на інформацію з посібників продуктів, часто задаваних питань (FAQ) та тикетів підтримки. Це може зменшити потребу у прямому людському втручанні для рутинних питань.

- **Персоналізовані рекомендації контенту:** Замість простого зіставлення ключових слів, RAG може виявляти та вилучати контент (статті, продукти), що семантично пов'язаний з уподобаннями користувача чи попередніми взаємодіями, приводячи до більш релевантних рекомендацій.

- **Суммування новин та поточних подій:** LLM можуть бути інтегровані з потоками новин у реальному часі. Коли запитується інформація про поточну подію, RAG-система вилучає недавні статті, дозволяючи LLM виробляти актуальне резюме.

Включаючи зовнішні знання, RAG розширює можливості LLM за межі простої розмови, дозволяючи їм функціонувати як системи обробки знань.

# Практичний приклад коду (ADK)

Щоб проілюструвати паттерн вилучення знань (RAG), давайте розглянемо три приклади.

По-перше, як використовувати Google Search для виконання RAG та обґрунтування результатів LLM результатами пошуку. Оскільки RAG включає доступ до зовнішної інформації, інструмент Google Search є прямим прикладом вбудованого механізму вилучення, що може доповнити знання LLM.

```python
from google.adk.tools import google_search
from google.adk.agents import Agent

search_agent = Agent(
    name="research_assistant",
    model="gemini-2.0-flash-exp",
    instruction="You help users research topics. When asked, use the Google Search tool",
    tools=[google_search]
)
```

По-друге, цей розділ пояснює, як використовувати можливості Vertex AI RAG у межах Google ADK. Наданий код демонструє ініціалізацію VertexAiRagMemoryService з ADK. Це дозволяє встановити з'єднання з Google Cloud Vertex AI RAG Corpus. Сервіс налаштовується шляхом визначення імені ресурсу corpus та додаткових параметрів, таких як SIMILARITY_TOP_K та VECTOR_DISTANCE_THRESHOLD. Ці параметри впливають на процес вилучення. SIMILARITY_TOP_K визначає кількість найбільш подібних результатів для вилучення. VECTOR_DISTANCE_THRESHOLD встановлює ліміт на семантичну відстань для вилучених результатів. Ця установка дозволяє агентам виконувати масштабоване та постійне семантичне вилучення знань з зазначеного RAG Corpus. Процес ефективно інтегрує функціональності RAG Google Cloud у агента ADK, таким чином підтримуючи розробку відповідей, засновано на фактичних даних.

```python
# Імпортуємо необхідний клас VertexAiRagMemoryService з модуля google.adk.memory.
from google.adk.memory import VertexAiRagMemoryService

RAG_CORPUS_RESOURCE_NAME = "projects/your-gcp-project-id/locations/us-central1/ragCorpora/your-corpus-id"

# Визначаємо додатковий параметр для кількості найбільш подібних результатів для вилучення.
# Це контролює, скільки релевантних шматків документів повертатиме RAG-сервіс.
SIMILARITY_TOP_K = 5

# Визначаємо додатковий параметр для порога векторної відстані.
# Цей поріг визначає максимальну семантичну відстань, дозволену для вилучених результатів;
# результати з відстанню, більшою за це значення, можуть бути відфільтровані.
VECTOR_DISTANCE_THRESHOLD = 0.7

# Ініціалізуємо екземпляр VertexAiRagMemoryService.
# Це встановлює з'єднання з вашим Vertex AI RAG Corpus.
# - rag_corpus: Визначає унікальний ідентифікатор для вашого RAG Corpus.
# - similarity_top_k: Встановлює максимальну кількість подібних результатів для вилучення.
# - vector_distance_threshold: Визначає поріг подібності для фільтрування результатів.
memory_service = VertexAiRagMemoryService(
    rag_corpus=RAG_CORPUS_RESOURCE_NAME,
    similarity_top_k=SIMILARITY_TOP_K,
    vector_distance_threshold=VECTOR_DISTANCE_THRESHOLD
)
```

# Практичний приклад коду (LangChain)

По-третє, давайте пройдемся через повний приклад, використовуючи LangChain.

```python
import os
import requests
from typing import List, Dict, Any, TypedDict
from langchain_community.document_loaders import TextLoader

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Weaviate
from langchain_openai import ChatOpenAI
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema.runnable import RunnablePassthrough
from langgraph.graph import StateGraph, END
import weaviate
from weaviate.embedded import EmbeddedOptions
import dotenv

# Завантажуємо змінні середовища (наприклад, OPENAI_API_KEY)
dotenv.load_dotenv()
# Встановлюємо ваш OpenAI API ключ (переконайтеся, що він завантажено з .env або встановлено тут)
# os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_API_KEY"

# --- 1. Підготовка даних (Попередня обробка) ---
# Завантажуємо дані
url = "https://github.com/langchain-ai/langchain/blob/master/docs/docs/how_to/state_of_the_union.txt"
res = requests.get(url)

with open("state_of_the_union.txt", "w") as f:
    f.write(res.text)


loader = TextLoader('./state_of_the_union.txt')
documents = loader.load()

# Розбиваємо документи на чанки
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = text_splitter.split_documents(documents)

# Вбудовуємо та зберігаємо чанки у Weaviate
client = weaviate.Client(
    embedded_options=EmbeddedOptions()
)

vectorstore = Weaviate.from_documents(
    client=client,
    documents=chunks,
    embedding=OpenAIEmbeddings(),
    by_text=False
)

# Визначаємо вилучувач
retriever = vectorstore.as_retriever()

# Ініціалізуємо LLM
llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0)

# --- 2. Визначаємо стан для LangGraph ---
class RAGGraphState(TypedDict):
    question: str
    documents: List[Document]
    generation: str

# --- 3. Визначаємо вузли (функції) ---

def retrieve_documents_node(state: RAGGraphState) -> RAGGraphState:
    """Вилучує документи на основі питання користувача."""
    question = state["question"]
    documents = retriever.invoke(question)
    return {"documents": documents, "question": question, "generation": ""}

def generate_response_node(state: RAGGraphState) -> RAGGraphState:
    """Генерує відповідь, використовуючи LLM на основі вилучених документів."""
    question = state["question"]
    documents = state["documents"]

    # Шаблон промпта з PDF
    template = """You are an assistant for question-answering tasks.
Use the following pieces of retrieved context to answer the question.
If you don't know the answer, just say that you don't know.
Use three sentences maximum and keep the answer concise.
Question: {question}
Context: {context}
Answer:
"""
    prompt = ChatPromptTemplate.from_template(template)

    # Форматуємо контекст з документів
    context = "\n\n".join([doc.page_content for doc in documents])

    # Створюємо RAG-ланцюжок
    rag_chain = prompt | llm | StrOutputParser()

    # Викликаємо ланцюжок
    generation = rag_chain.invoke({"context": context, "question": question})
    return {"question": question, "documents": documents, "generation": generation}

# --- 4. Будуємо граф LangGraph ---

workflow = StateGraph(RAGGraphState)

# Додаємо вузли
workflow.add_node("retrieve", retrieve_documents_node)
workflow.add_node("generate", generate_response_node)

# Встановлюємо точку входу
workflow.set_entry_point("retrieve")

# Додаємо ребра (переходи)
workflow.add_edge("retrieve", "generate")
workflow.add_edge("generate", END)

# Компілюємо граф
app = workflow.compile()

# --- 5. Запускаємо RAG-додаток ---
if __name__ == "__main__":
    print("\n--- Запуск RAG-запиту ---")
    query = "What did the president say about Justice Breyer"
    inputs = {"question": query}
    for s in app.stream(inputs):
        print(s)

    print("\n--- Запуск іншого RAG-запиту ---")
    query_2 = "What did the president say about the economy?"
    inputs_2 = {"question": query_2}
    for s in app.stream(inputs_2):
        print(s)
```

Цей Python-код ілюструє конвеєр Retrieval-Augmented Generation (RAG), впроваджений з допомогою LangChain та LangGraph. Процес починається з створення бази знань, отриманої з текстового документа, який сегментується на чанки та трансформується у вбудовування. Ці вбудовування потім зберігаються у векторному сховищі Weaviate, полегшуючи ефективне вилучення інформації. StateGraph у LangGraph використовується для управління робочим процесом між двома ключовими функціями: `retrieve_documents_node` та `generate_response_node`. Функція `retrieve_documents_node` запитує векторне сховище для ідентифікації релевантних чанків документів на основі введення користувача. Як наслідок, функція `generate_response_node` використовує вилучену інформацію та попередньо визначений шаблон промпта для створення відповіді, використовуючи OpenAI Large Language Model (LLM). Метод `app.stream` дозволяє виконання запитів через конвеєр RAG, демонструючи здатність системи генерувати контекстуально релевантні результати.

# З першого погляду

**Що:** LLM мають вражаючі здібності у генеруванні тексту, але принципово обмежені своїми даними тренування. Ці знання є статичними, тобто не включають інформацію в реальному часі чи приватні, специфічні для домену дані. Отже, їхні відповіді можуть бути застарілими, неточними чи позбавленими специфічного контексту, необхідного для спеціалізованих завдань. Цей розрив обмежує їхню надійність для додатків, що вимагають актуальних та фактичних відповідей.

**Чому:** Паттерн Retrieval-Augmented Generation (RAG) надає стандартизоване рішення, з'єднуючи LLM з зовнішніми джерелами знань. Коли отримано запит, система спочатку вилучає релевантні фрагменти інформації з зазначеної бази знань. Ці фрагменти потім додаються до оригінального промпта, збагачуючи його своєчасним та специфічним контекстом. Цей дополнений промпт потім надсилається до LLM, дозволяючи йому генерувати відповідь, яка точна, перевіряється та засновано на зовнішніх даних. Цей процес ефективно трансформує LLM з рассудження "із закритою книгою" на рассудження "з відкритою книгою", значно підвищуючи його корисність та надійність.

**Емпіричне правило:** Використовуйте цей паттерн, коли вам потрібно, щоб LLM відповідав на питання чи генерував контент на основі специфічної, актуальної чи закритої інформації, яка не була частиною його оригінальних даних тренування. Він ідеальний для побудови систем запитання-відповіді по внутрішнім документам, ботів підтримки клієнтів та додатків, що вимагають перевіряємих, засновано на фактах відповідей з цитатами.

**Візуальне резюме**

![][image3]

Паттерн вилучення знань: AI-агент для запиту та вилучення інформації зі структурованих баз даних

![][image4]

Рис. 3: Паттерн вилучення знань: AI-агент для пошуку та синтезу інформації з публічного інтернету у відповідь на запити користувачів.

# Ключові висновки

- Вилучення знань (RAG) розширює можливості LLM, дозволяючи їм отримувати доступ до зовнішної, актуальної та специфічної інформації.
- Процес включає вилучення (пошук у базі знань релевантних фрагментів) та доповнення (додавання цих фрагментів до промпта LLM).
- RAG допомагає LLM долати обмеження, такі як застарілі дані тренування, зменшує "галюцинації" та забезпечує інтеграцію специфічних для домену знань.
- RAG дозволяє атрибутовані відповіді, оскільки відповідь LLM засновано на вилучених джерелах.
- GraphRAG використовує граф знань для розуміння взаємозв'язків між різними фрагментами інформації, дозволяючи відповідати на складні питання, що вимагають синтезу даних з кількох джерел.
- Agentic RAG виходить за межи простого вилучення інформації, використовуючи розумного агента для активного рассудження, валідації та уточнення зовнішніх знань, забезпечуючи більш точну та надійну відповідь.
- Практичні застосування охоплюють корпоративний пошук, підтримку клієнтів, юридичні дослідження та персоналізовані рекомендації.

# Висновок

На завершення, Retrieval-Augmented Generation (RAG) розв'язує базове обмеження статичних знань Large Language Model, з'єднуючи їх з зовнішніми, актуальними джерелами даних. Процес працює шляхом першопочаткового вилучення релевантних фрагментів інформації та подальшого доповнення промпта користувача, дозволяючи LLM генерувати більш точні та контекстуально обізнані відповіді. Це стає можливим завдяки фундаментальним технологіям, таким як вбудовування, семантичний пошук та векторні бази даних, що знаходять інформацію на основі значення, а не тільки ключових слів. Засновуючи результати на перевіряємих даних, RAG значно зменшує фактичні помилки та дозволяє використання закритої інформації, підвищуючи довіру через цитати.

Продвинута еволюція, Agentic RAG, вводить шар рассудження, що активно валідирує, узгоджує та синтезує вилучені знання для ще більшої надійності. Подібно, спеціалізовані підходи, такі як GraphRAG, використовують графи знань для навігації явними взаємозв'язками даних, дозволяючи системі синтезувати відповіді на надзвичайно складні, взаємопов'язані запити. Цей агент може розв'язувати конфліктуючу інформацію, виконувати багатоступеневі запити та використовувати зовнішні інструменти для пошуку відсутніх даних. Хоча ці продвинуті методи додають складність та затримку, вони драматично поліпшують глибину та надійність остаточної відповіді. Практичні застосування цих паттернів вже трансформують галузі, від корпоративного пошуку та підтримки клієнтів до персоналізованої доставки контенту. Незважаючи на проблеми, RAG є критичним паттерном для підвищення знань, надійності та корисності AI. Зрештою, він трансформує LLM з розмовників із закритою книгою на потужні інструменти рассудження з відкритою книгою.

# Література

1. Lewis, P., et al. (2020). _Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks_. https://arxiv.org/abs/2005.11401
2. Google AI for Developers Documentation. _Retrieval Augmented Generation_ - https://cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/rag-overview
3. [Retrieval-Augmented Generation with Graphs (GraphRAG)](https://arxiv.org/abs/2501.00309)
4. LangChain and LangGraph: Leonie Monigatti, "Retrieval-Augmented Generation (RAG): From Theory to LangChain Implementation," https://medium.com/data-science/retrieval-augmented-generation-rag-from-theory-to-langchain-implementation-4e9bd5f6a4f2
5. Google Cloud Vertex AI RAG Corpus https://cloud.google.com/vertex-ai/generative-ai/docs/rag-engine/manage-your-rag-corpus#corpus-management

[image1]: ../Assets/chapter-14-image1.png
[image2]: ../Assets/chapter-14-image2.png
[image3]: ../Assets/chapter-14-image3.png
[image4]: ../Assets/chapter-14-image4.png

---

## Навігація

**Назад:** [Розділ 13. Людина в контурі](Розділ%2013.%20Людина%20в%20контурі.md)<br>
**Вперед:** [Розділ 15. Міжагентна комунікація](../Частина%204/Розділ%2015.%20Міжагентна%20комунікація.md)
