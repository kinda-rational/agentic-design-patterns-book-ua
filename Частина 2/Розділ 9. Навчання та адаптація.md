# Розділ 9: Навчання та адаптація

Навчання та адаптація є ключовими факторами для покращення можливостей агентів штучного інтелекту. Ці процеси дозволяють агентам розвиватися за межи попередньо визначених параметрів, даючи їм можливість автономно поліпшуватися через досвід та взаємодію з навколишнім середовищем. Завдяки навчанню та адаптації агенти можуть ефективно справляти з новими ситуаціями та оптимізувати свою продуктивність без постійного ручного втручання. У цьому розділі детально розглядаються принципи та механізми, які лежать в основі навчання та адаптації агентів.

## Загальна картина

Агенти навчаються та адаптуються, змінюючи своє мислення, дії або знання на основі нового досвіду та даних. Це дозволяє агентам еволюціонувати від простого дотримання інструкцій до того, щоб з часом ставати розумнішими.

- **Навчання з підкріпленням:** Агенти пробують дії та отримують винагороди за позитивні результати та штрафи за негативні, вивчаючи оптимальну поведінку в мінливих ситуаціях. Корисно для агентів, що керують роботами або грають у гри.

- **Навчання з вчителем:** Агенти вчаться на позначених прикладах, пов'язуючи вхідні дані з бажаними вихідними результатами, що дозволяє виконувати завдання прийняття рішень та розпізнавання шаблонів. Ідеально для агентів, які сортують електронну пошту або передбачають тренди.

- **Навчання без вчителя:** Агенти виявляють приховані зв'язки та шаблони в непозначених даних, допомагаючи отримати інсайти, організацію та створення ментальної карти свого середовища. Корисно для агентів, що досліджують дані без конкретного керівництва.

- **Навчання з кількома/нульовими прикладами з агентами на основі LLM:** Агенти, що використовують LLM, можуть швидко адаптуватися до нових завдань з мінімальними прикладами або чіткими інструкціями, забезпечуючи швидкі реакції на нові команди або ситуації.

- **Навчання онлайн:** Агенти безперервно оновлюють знання новими даними, що критично важливо для реакцій у реальному часі та постійної адаптації в динамічних середовищах. Критично важливо для агентів, що обробляють безперервні потоки даних.

- **Навчання на основі пам'яті:** Агенти згадують минулий досвід, щоб коригувати поточні дії в подібних ситуаціях, покращуючи контекстну обізнаність і прийняття рішень. Ефективно для агентів із можливостями виклику пам'яті.

Агенти адаптуються, змінюючи стратегію, розуміння або цілі на основі навчання. Це життєво важливо для агентів у непередбачуваних, мінливих або нових середовищах.

**Проксимальна оптимізація політики (PPO)** — це алгоритм навчання з підкріпленням, який використовується для навчання агентів у середовищах з безперервним діапазоном дій, таких як управління суглобами робота або персонажем у грі. Його основна мета — надійно та стабільно поліпшувати стратегію прийняття рішень агента, яка відома як його політика.

Основна ідея PPO полягає у внесенні невеликих, обережних оновлень до політики агента. Він уникає різких змін, які можуть призвести до колапсу продуктивності. Ось як це працює:

1. Збір даних: Агент взаємодіє зі своїм середовищем (наприклад, грає в гру), використовуючи свою поточну політику, і збирає пакет досвіду (стан, дія, винагорода).

2. Оцінка "суррогатної" мети: PPO розраховує, як потенціальне оновлення політики змінить очікувану винагороду. Однак замість простої максимізації цієї винагороди він використовує спеціальну "обрізану" цільову функцію.

3. Механізм "обрізання": Це ключ до стійкості PPO. Він створює "область довіри" або безпечну зону навколо поточної політики. Алгоритм запобігає внесенню оновлення, яке занадто відрізняється від поточної стратегії. Це обрізання діє як передохоронний гальмо, гарантуючи, що агент не робить величезний, ризикований крок, який скасовує його навчання.

Коротко кажучи, PPO балансує покращення продуктивності зі збереженням близькості до відомої, робочої стратегії, що запобігає катастрофічним відмовам під час навчання і призводить до більш стабільного навчання.

**Пряма оптимізація переваги (DPO)** — це новіший метод, спеціально розроблений для вирівнювання великих мовних моделей (LLM) з людськими перевагами. Він пропонує простішу, більш пряму альтернативу використанню PPO для цього завдання.

Для розуміння DPO корисно спочатку розуміти традиційний метод вирівнювання на основі PPO:

- **Підхід PPO (двоетапний процес):**

1. Навчання моделі винагороди: Спочатку ви збираєте дані зворотного зв'язку від людей, де люди оцінюють або порівнюють різні відповіді LLM (наприклад, "Відповідь A краща, ніж відповідь B"). Ці дані використовуються для навчання окремої AI моделі, яка називається моделлю винагороди, задача якої — передбачити, яку оцінку людина дала б будь-якій новій відповіді.

2. Тонке налаштування за допомогою PPO: Потім LLM тонко налаштовується за допомогою PPO. Мета LLM — генерувати відповіді, які отримують найвищий можливий бал від моделі винагороди. Модель винагороди діє як "суддя" в грі навчання.

Цей двоетапний процес може бути складним і нестабільним. Наприклад, LLM може знайти шпару та навчитися "розламувати" модель винагороди, щоб отримати високі бали за погані відповіді.

- **Підхід DPO (прямий процес):** DPO повністю пропускає модель винагороди. Замість перетворення людських переваг у бал винагороди і подальшої оптимізації для цього балу, DPO використовує дані переваг безпосередньо для оновлення політики LLM.

- Він працює, використовуючи математичний зв'язок, який безпосередньо пов'язує дані переваг з оптимальною політикою. По суті, він учить модель: "Збільш ймовірність генерування відповідей, подібних до переважаних, і зменш ймовірність генерування подібних небажаним."

По суті, DPO спрощує вирівнювання, безпосередньо оптимізуючи мовну модель на даних людських переваг. Це дозволяє уникнути складності та потенційної нестабільності навчання та використання окремої моделі винагороди, роблячи процес вирівнювання більш ефективним та надійним.

## Практичні застосування та випадки використання

Адаптивні агенти демонструють покращену продуктивність в мінливих середовищах через ітеративні оновлення, керовані досвідом.

- **Агенти персоналізованих помічників** вдосконалюють протоколи взаємодії через довгострокове аналізування поведінки окремих користувачів, забезпечуючи високооптимізоване генерування відповідей.

- **Агенти торгівельних ботів** оптимізують алгоритми прийняття рішень, динамічно коригуючи параметри моделі на основі точних ринкових даних у реальному часі, тим самим максимізуючи фінансовий дохід та мінімізуючи ризики.

- **Агенти додатків** оптимізують користувацький інтерфейс та функціональність через динамічні модифікації на основі спостережуваної поведінки користувачів, що призводить до збільшення залучення користувачів та інтуїтивності системи.

- **Агенти робототехніки та автономних транспортних засобів** вдосконалюють навігаційні та реактивні можливості, інтегруючи дані датчиків та аналіз історичних дій, забезпечуючи безпечну та ефективну роботу в різноманітних умовах навколишнього середовища.

- **Агенти виявлення шахрайства** поліпшують виявлення аномалій, вдосконалюючи прогностичні моделі за допомогою новоповідомлених шахрайських схем, підвищуючи безпеку системи та мінімізуючи фінансові збитки.

- **Агенти рекомендацій** підвищують точність вибору контенту, використовуючи алгоритми вивчення користувацьких переваг, забезпечуючи високо персоналізовані та контекстуально релевантні рекомендації.

- **Агенти ігрового AI** підвищують залучення гравців, динамічно адаптуючи стратегічні алгоритми, тим самим збільшуючи складність та виклик гри.

- **Агенти навчання бази знань:** Агенти можуть використовувати генерацію з доповненням пошуку (RAG) для підтримання динамічної бази знань описів проблем та перевірених рішень (див. розділ 14). Зберігаючи успішні стратегії та зустрінуті завдання, агент може посилатися на ці дані під час прийняття рішень, дозволяючи йому більш ефективно адаптуватися до нових ситуацій, застосовуючи раніше успішні схеми або уникаючи відомих пасток.

## Кейс-стаді: Самозавдячуючийся агент програмування (SICA)

Самозавдячуючийся агент програмування (SICA), розроблений Максимом Робейнсом, Лоуренсом Айчисоном та Мартином Саммером, являє собою досягнення в галузі агентного навчання, демонструючи здатність агента модифікувати свій власний вихідний код. Це контрастує з традиційними підходами, де один агент може навчати іншого; SICA діє як модифікатор та модифікована сутність одночасно, ітеративно вдосконалюючи свою кодову базу для покращення продуктивності в різних завданнях програмування.

Самозавдячення SICA працює через ітеративний цикл (див. мал. 1). Спочатку SICA переглядає архив своїх попередніх версій та їхнього виконання на еталонних тестах. Він вибирає версію з найвищою оцінкою продуктивності, розрахованої на основі зваженої формули, що враховує успіх, час та обчислювальні витрати. Ця вибрана версія потім здійснює наступний раунд самомодифікації. Вона аналізує архив для виявлення потенційних вдосконалень, а потім безпосередньо змінює свою кодову базу. Модифікований агент потім тестується проти еталонів, з записуванням результатів в архив. Цей процес повторюється, сприяючи навчанню безпосередньо з минулої продуктивності. Цей механізм самовдосконалення дозволяє SICA розвивати свої можливості без потреби в традиційних парадигмах навчання.

![][image1]

Мал. 1: Самовдосконалення SICA, навчання та адаптація на основі своїх попередніх версій

SICA пройшов значне самовдосконалення, що призвело до досягнень в редагуванні коду та навігації. Спочатку SICA використовував базовий підхід переписування файлів для змін коду. Згодом він розробив "Розумний редактор", здатний до більш інтелектуальних та контекстуальних редагувань. Це еволюціонувало в "Розумний редактор з поліпшенням Diff", включаючи дифи для цільових модифікацій та редагування на основі шаблонів, та "Інструмент швидкого переписування" для зменшення потреб у обробці.

SICA далі реалізував "Оптимізацію мінімального виведення Diff" та "Контекстно-чутливу мінімізацію Diff", використовуючи парсинг абстрактного синтаксичного дерева (AST) для ефективності. Крім того, був доданий "Нормалізатор введення SmartEditor". Щодо навігації SICA незалежно створив "Локатор символів AST", використовуючи структурну карту коду (AST) для ідентифікації визначень у кодовій базі. Пізніше був розроблений "Гібридний локатор символів", що поєднує швидкий пошук з перевіркою AST. Це було далі оптимізовано через "Оптимізований парсинг AST в гібридному локаторі символів" для фокусу на релевантних секціях коду, поліпшуючи швидкість пошуку (див. мал. 2).

![][image4]

Мал. 2: Продуктивність за ітераціями. Ключові вдосконалення анотовані з їхніми відповідними модифікаціями інструментів або агентів. (будь ласка, дякую Максимові Робейнсу, Мартину Саммеру, Лоуренсу Айчисону)

Архітектура SICA включає базовий інструментарій для основних файлових операцій, виконання команд та арифметичних обчислень. Вона включає механізми для відправлення результатів та виклику спеціалізованих під-агентів (програмування, розв'язання проблем та розумування). Ці під-агенти розкладають складні завдання та керують довжиною контексту LLM, особливо під час розширених циклів вдосконалення.

Асинхронний спостерігач, інший LLM, моніторит поведінку SICA, виявляючи потенційні проблеми, такі як цикли або застій. Він спілкується з SICA та може втрутитися, щоб зупинити виконання при необхідності. Спостерігач отримує детальний звіт про дії SICA, включаючи граф викликів та журнал повідомлень та дій інструментів, для виявлення шаблонів та неефективностей.

LLM SICA організує інформацію в своєму контекстному вікні, своїй краткосрочній пам'яті, структурованим чином, критично важливим для його роботи. Ця структура включає системний промпт, що визначає цілі агента, документацію інструментів та під-агентів та системні інструкції. Основний промпт містить постановку проблеми або інструкцію, вміст відкритих файлів та карту каталогів. Повідомлення асистента записують поетапне розумування агента, записи викликів інструментів та під-агентів та їхні результати, та комунікацію спостерігача. Ця організація сприяє ефективному потоку інформації, поліпшуючи роботу LLM та зменшуючи час обробки та витрати. Спочатку змінення файлів записувалися як дифи, що показували лише модифікації та періодично консолідувалися.

**SICA: Поглиблений погляд на код:** Більш глибокий аналіз реалізації SICA розкриває кілька ключових проектних рішень, що лежать в основі його можливостей. Як обговорювалося, система побудована з модульною архітектурою, включаючи кількох під-агентів, таких як агент програмування, агент розв'язання проблем та агент розумування. Ці під-агенти викликаються головним агентом, подібно викликам інструментів, служачи для розкладання складних завдань та ефективного управління довжиною контексту, особливо під час тих розширених ітерацій мета-вдосконалення.

Проект активно розробляється та прагне надати надійну платформу для тих, хто зацікавлений в пост-навчанні LLM для використання інструментів та інших агентних завдань, з повним кодом, доступним для подальшого дослідження та внеску в репозитаріях GitHub https://github.com/MaximeRobeyns/self_improving_coding_agent/.

Для безпеки проект сильно наголошує на контейнеризації Docker, означаючи, що агент працює в виділеному контейнері Docker. Це критична мера, оскільки вона забезпечує ізоляцію від хост-машини, зменшуючи ризики, такі як непередбачувана маніпуляція файловою системою, враховуючи здатність агента виконувати команди shell.

Для забезпечення прозорості та контролю система має надійну спостережуваність через інтерактивну веб-сторінку, яка візуалізує події на шині подій та граф викликів агента. Це пропонує всеохопні інсайти в дії агента, дозволяючи користувачам інспектувати окремі події, читати повідомлення спостерігача та складати трасування під-агентів для чіткішого розуміння.

Щодо свого основного інтелекту, фреймворк агента підтримує інтеграцію LLM від різних постачальників, дозволяючи експериментувати з різними моделями для знаходження найкращого відповідності для конкретних завдань. Нарешті, критичним компонентом є асинхронний спостерігач, LLM, який працює паралельно з головним агентом. Цей спостерігач періодично оцінює поведінку агента на патологічні відхилення або застій та може втрутитися, відправляючи сповіщення або навіть скасовуючи виконання агента при необхідності. Він отримує детальне текстове представлення стану системи, включаючи граф викликів та потік подій LLM повідомлень, викликів інструментів та відповідей, що дозволяє йому виявляти неефективні шаблони або повторювальну роботу.

Помітною проблемою в початковій реалізації SICA було спонукання агента на основі LLM незалежно пропонувати нові, інноваційні, здійснювані та привабливі модифікації під час кожної ітерації мета-вдосконалення. Це обмеження, особливо в спонуканні відкритого навчання та справжньої креативності в агентах LLM, залишається ключовою областю досліджень у поточних дослідженнях.

## AlphaEvolve та OpenEvolve

**AlphaEvolve** — це агент AI, розроблений Google для виявлення та оптимізації алгоритмів. Він використовує комбінацію LLM, конкретно моделей Gemini (Flash та Pro), автоматизованих систем оцінки та фреймворку еволюційних алгоритмів. Ця система прагне просунути як теоретичну математику, так і практичні обчислювальні програми.

AlphaEvolve використовує ансамбль моделей Gemini. Flash використовується для генерування широкого спектру початкових пропозицій алгоритмів, тоді як Pro забезпечує глибший аналіз та доопрацювання. Запропоновані алгоритми потім автоматично оцінюються та оцінюються на основі попередньо визначених критеріїв. Ця оцінка забезпечує зворотний зв'язок, який використовується для ітеративного поліпшення рішень, призводячи до оптимізованих та нових алгоритмів.

У практичних обчисленнях AlphaEvolve був розгорнутий на інфраструктурі Google. Він продемонстрував поліпшення в плануванні центрів обробки даних, що призвело до зменшення використання глобальних обчислювальних ресурсів на 0,7%. Він також сприяв дизайну обладнання, пропонуючи оптимізації для коду Verilog у найближчих блоках процесорів Tensor (TPU). Крім того, AlphaEvolve пришвидшив продуктивність AI, включаючи поліпшення швидкості на 23% в основному ядрі архітектури Gemini та до 32,5% оптимізації низькорівневих інструкцій GPU для FlashAttention.

У галузі фундаментальних досліджень AlphaEvolve сприяв відкриттю нових алгоритмів для множення матриць, включаючи метод для матриць 4x4 зі складними значеннями, що використовує 48 скалярних множень, перевершуючи раніше відомі рішення. У ширших математичних дослідженнях він переткав існуючі сучасні рішення для більш ніж 50 відкритих проблем у 75% випадків та поліпшив існуючі рішення у 20% випадків, з прикладами, включаючи досягнення в проблемі поцілунку.

**OpenEvolve** — це еволюційний агент програмування, який використовує LLM (див. мал. 3) для ітеративної оптимізації коду. Він оркеструє конвеєр генерування коду, керований LLM, оцінки та вибору для безперервного поліпшення програм для широкого спектру завдань. Ключовим аспектом OpenEvolve є його здатність еволюціонувати цілі файли коду, а не обмежуватися окремими функціями. Агент розроблений для універсальності, пропонуючи підтримку кількох мов програмування та сумісність з API-сумісними з OpenAI для будь-якого LLM. Крім того, він включає багатоцільову оптимізацію, дозволяє гнучку інженерію промптів та здатний до розподіленої оцінки для ефективної обробки складних завдань програмування.

![][image2]

Мал. 3: Внутрішня архітектура OpenEvolve керується контролером. Цей контролер оркеструє кілька ключових компонентів: семпер програм, базу даних програм, пул оцінювачів та ансамблі LLM. Його основна функція — сприяти їхнім процесам навчання та адаптації для поліпшення якості коду.

Цей фрагмент коду використовує бібліотеку OpenEvolve для виконання еволюційної оптимізації програми. Він ініціалізує систему OpenEvolve із шляхами до початкової програми, файлу оцінки та файлу конфігурації. Рядок evolve.run(iterations=1000) запускає еволюційний процес, виконуючи 1000 ітерацій для знаходження поліпшеної версії програми. Нарешті, вона виводить метрики найкращої програми, знайденої під час еволюції, відформатованої до чотирьох знаків після коми.

```python
from openevolve import OpenEvolve

# Initialize the system
evolve = OpenEvolve(
    initial_program_path="path/to/initial_program.py",
    evaluation_file="path/to/evaluator.py",
    config_path="path/to/config.yaml"
)

# Run the evolution
best_program = await evolve.run(iterations=1000)
print(f"Best program metrics:")
for name, value in best_program.metrics.items():
    print(f"  {name}: {value:.4f}")
```

## В двох словах

**Що:** Агенти AI часто працюють в динамічних та непередбачуваних середовищах, де попередньо запрограмована логіка недостатня. Їхня продуктивність може деградувати при зіткненні з новими ситуаціями, не передбаченими під час їхнього початкового дизайну. Без здатності навчатися з досвіду агенти не можуть оптимізувати свої стратегії або персоналізувати свої взаємодії з часом. Ця жорсткість обмежує їхню ефективність та запобігає досягненню справжної автономії в складних, реальних сценаріях.

**Чому:** Стандартизоване рішення — інтегрувати механізми навчання та адаптації, трансформуючи статичних агентів в динамічні, еволюціонуючі системи. Це дозволяє агенту автономно вдосконалювати свої знання та поведінку на основі нових даних та взаємодій. Агентні системи можуть використовувати різноманітні методи, від навчання з підкріпленням до більш передових методик, таких як самомодифікація, як видно в самозавдячуючомуся агенті програмування (SICA). Передові системи, такі як Google AlphaEvolve, використовують LLM та еволюційні алгоритми для відкриття абсолютно нових та більш ефективних рішень складних проблем. Безперервно навчаючись, агенти можуть оволодівати новими завданнями, поліпшувати свою продуктивність та адаптуватися до змінюваних умов без потреби в постійному ручному перепрограмуванні.

**Практичне правило:** Використовуйте цей паттерн при побудові агентів, які повинні працювати в динамічних, невизначених або еволюціонуючих середовищах. Він необхідний для додатків, що потребують персоналізації, безперервного поліпшення продуктивності та здатності автономно керувати новими ситуаціями.

**Візуальне резюме**

![][image3]

Мал. 4: Паттерн навчання та адаптації

## Ключові висновки

- Навчання та адаптація стосуються того, як агенти стають кращими в тому, що вони роблять, та обробляють нові ситуації, використовуючи свій досвід.

- "Адаптація" — це видима зміна в поведінці або знаннях агента, яка виникає з навчання.

- SICA, самозавдячуючийся агент програмування, самовдосконалюється, модифікуючи свій код на основі минулої продуктивності. Це призвело до інструментів, таких як розумний редактор та локатор символів AST.

- Наявність спеціалізованих "під-агентів" та "спостерігача" допомагає цим самовдосконалюючимся системам керувати великими завданнями та залишатися на правильному курсі.

- Спосіб організації "контекстного вікна" LLM (із системними промптами, основними промптами та повідомленнями асистента) має критичне значення для того, наскільки ефективно працюють агенти.

- Цей паттерн життєво важливий для агентів, що повинні працювати в середовищах, які постійно змінюються, невизначені або потребують персонального підходу.

- Побудова навчаючих агентів часто означає підключення їх до інструментів машинного навчання та управління потоком даних.

- Агентна система, обладнана базовими інструментами програмування, може автономно редагувати себе й таким чином поліпшувати свою продуктивність у еталонних завданнях.

- AlphaEvolve — це агент AI від Google, що використовує LLM та еволюційний фреймворк для автономного відкриття та оптимізації алгоритмів, значно поліпшуючи як фундаментальні дослідження, так і практичні обчислювальні програми.

## Висновок

У цьому розділі розглядаються критично важливі ролі навчання та адаптації в штучному інтелекті. Агенти AI поліпшують свою продуктивність через безперервне отримання даних та досвіду. Самозавдячуючийся агент програмування (SICA) ілюструє це, автономно вдосконалюючи свої можливості через модифікації коду.

Ми розглянули фундаментальні компоненти агентного AI, включаючи архітектуру, програми, планування, багатоагентне співробітництво, управління пам'яттю та навчання та адаптацію. Принципи навчання особливо важливі для узгодженого поліпшення в багатоагентних системах. Для досягнення цього налаштування даних повинні точно відображати повну траєкторію взаємодії, захоплюючи окремі входи та виходи кожного задіяного агента.

Ці елементи сприяють значним досягненням, таким як Google AlphaEvolve. Ця система AI незалежно відкриває та вдосконалює алгоритми за допомогою LLM, автоматизованої оцінки та еволюційного підходу, просуваючи прогрес у наукових дослідженнях та обчислювальних техніках. Такі паттерни можна поєднувати для побудови складних систем AI. Розробки, такі як AlphaEvolve, демонструють, що автономне алгоритмічне відкриття та оптимізація агентами AI досяжні.

## Посилання

1. Sutton, R. S., & Barto, A. G. (2018). _Reinforcement Learning: An Introduction_. MIT Press.

2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). _Deep Learning_. MIT Press.

3. Mitchell, T. M. (1997). _Machine Learning_. McGraw-Hill.

4. Proximal Policy Optimization Algorithms by John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. You can find it on arXiv: https://arxiv.org/abs/1707.06347

5. Robeyns, M., Aitchison, L., & Szummer, M. (2025). _A Self-Improving Coding Agent_. arXiv:2504.15228v2. https://arxiv.org/pdf/2504.15228 https://github.com/MaximeRobeyns/self_improving_coding_agent

6. [AlphaEvolve blog](https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/)

7. [OpenEvolve](https://github.com/codelion/openevolve)

[image1]: ../Assets/chapter-9-image1.png
[image2]: ../Assets/chapter-9-image2.png
[image3]: ../Assets/chapter-9-image3.png
[image4]: ../Assets/chapter-9-image4.png

---

## Навігація

**Назад:** [Розділ 8. Управління пам'яттю](Розділ%208.%20Управління%20пам'яттю.md)
**Вперед:** [Розділ 10. Протокол контексту моделі](Розділ%2010.%20Протокол%20контексту%20моделі.md)
